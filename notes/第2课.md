# 书生浦语大模型训练营--第二课

## 什么是大模型

- 人工智能领域中参数数量巨大、拥有庞大计算能力和参数规模的模型。
- 特点及应用。利用大量数据进行训练，拥有数十亿甚至数干亿个参数，模型在各种任务中展现出惊人的性能

##  InternLM 模型全链条开源

- InternLM 是一个开源的轻量级**训练框架**。基于InternLM训练框架，**上海人工智能实验室**已经发布了两个开源的**预训练模型**:InternLM-7B 和 InternLM-20B

## InternLM-Chat-7B智能对话demo

InternLm-7B包括了两个部分：70亿参数的基础模型，和一个对话模型。该模型使用数万亿token训练，支持8K token的上下文。

## Lagent智能体工具调用demo

Lagent 是一个轻量级、开源的基于大语言模型的智能体(agent)框架，用户可以快速地将一个大语言模型转变为多种类型的智能体。

## 浦语灵笔图文创作理解demo

浦语·灵笔是基于书生·浦语大语言模型研发的 视觉·语言 大模型，拥有强大图文理解和创作能力。

## 通用环境配置

### pip换源

```shell
#升级pip到最新版本(>=10.0.0)后进行配置
python -m pip install --upgrade pip
#设置pip镜像源
pip config set global.index-url https://mirrors.cernet.edu.cn/pypi/web/simple
```

- 如果pip默认源的网络连接较差，临时使用镜像源升级pip

  `python -m pip install -i https://mirrors.cernet.edu.cn/pypi/web/simple --upgrade pip`

### conda换源

> 通过修改用户目录下的 `.condarc` 文件来使用镜像站。

不同系统下的`.condarc`目录如下

- `Linux`: `${HOME}/.condarc`
- `macOS`: `${HOME}/.condarc`
- `Windows`: `C:\Users\<YourUserName>\.condarc`

注意：

- `Windows` 用户无法直接创建名为 `.condarc` 的文件，可先执行 `conda config --set show_channel_urls yes` 生成该文件之后再修改。

在终端执行如果操作可将配置写入`.condarc`文件

```shell
cat <<'EOF' > ~/.condarc
channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
EOF
```

模型下载：

- Hugging face-cli命令行工具。

  ```python
  pip install -U huggingface_hub
  huggingface-cli download --resume-download internlm/internlm-chat-7b --local-dir your_path
  ```
- OpenXLab 可以通过指定模型仓库的地址，以及需要下载的文件的名称，文件所需下载的位安装代码如下:pip置等，
  直接下载模型权重文件。使用 Python 脚本下载模型首先要安装依赖`install -U openxlab `安装完成后
  使用 `download` 函数导入模型中心的模型将以下代码写入 `Python` 文件，运行即可。
  ```python
  from openxlab.model import download
  download(model _repo='openLMLab/InternLM-7b', model name='InternLM-7b', output='your local path')
  ```
  
### 将服务器端口映射到本地

> 由于服务器只暴露的用于远程登录的SSH端口，如果在服务器上运行了其他服务，比如一个可以接受请求的web应用，我们需要访问这个服务就需要使用SSH隧道将服务器特定端口映射到本地计算机端口。

也就是说，我们的开发机只暴露了SSH远程登录端口`:22`，我们请求服务器时只能请求22端口进行SSH连接，即使服务器上运行了一个`web`应用，由于服务器没有暴露web服务监听的端口所以无法请求到服务器上的web服务。但是我们可以使用`ssh`隧道技术，把服务器上的端口通过`ssh`隧道映射到本地计算机，这样如果想请求服务器上的web服务，比如这里启动的web demo，只需要访问本地映射的端口即可。

#### 踩过的坑

mac上使用命令`ssh-keygen -t rsa -b 4096 -C "your email@example.com" `生成密钥，并且配置公钥到服务器即可。一般的教程都会教给大家这么做，可是如果你之前生成过ssh密钥，本地的`~/.ssh`文件中就会存在多个密钥对，再执行端口映射的指令可能会出错（如果第一次生成密钥可能不会，不配置config会根据默认规则寻找私钥文件）。这里需要大家在ssh的config文件做一些配置指定在该开发机上进行ssh连接使用的私钥文件，打开`~/.ssh/config`文件，没有则创建一个，在配置中添加下面的配置

```shell
Host ssh.intern-ai.org.cn 
  HostName ssh.intern-ai.org.cn #这个是开发机的域名
  IdentityFile ~/.ssh/id_rsa_intern #ssh连接到开发机，指定的私钥文件，填写你生成的私钥文件
  PreferredAuthentications publickey
  StrictHostKeyChecking no
  Port 34033 #这个端口是开发机暴露的ssh连接端口
  User root
```

配置完之后在执行SSH隧道映射命令:`ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 34033`-p后面指定的是开发机，暴露的端口和config文件中`Port`端口一致。相关的介绍我写了一篇[博客](https://juejin.cn/post/7320655390135009318)。

使用 `huggingface_hub` 下载模型时需要指定`--repo_id`参数，该参数是模型所在的仓库的ID，
在模型中心的模型详情页面，会显示该参数，如`openLMLab/InternLM-20b`，
在命令行中使用该参数指定模型所在的仓库即可。
使用 `filename` 参数指定下载文件名，如`config.json`。
`cache_dir` 参数指定模型下载的缓存目录，如`./cache`。如果下载中遇到下面报错

```shell
  huggingface_hub.utils._errors.LocalEntryNotFoundError: 
  An error happened while trying to locate the file on 
  the Hub and we cannot find the requested files 
  in the local cache. Please check your connection and try again 
  or make sure your Internet connection is on.
```

说明huggingface的下载源有问题，可以指定huggingface镜像源,具体的方法在[这里](https://hf-mirror.com/)
